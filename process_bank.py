{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c7f1f-e60a-47ad-8653-b0c78a0c589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import (cross_validate, cross_val_predict,\n",
    "                                     train_test_split)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "def split_data(\n",
    "        raw_df: pd.DataFrame,\n",
    "        target_col: str\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split the raw dataframe into training and test sets.\n",
    "\n",
    "    Args:\n",
    "        raw_df (pd.DataFrame): The raw dataframe.\n",
    "        target_col (str): Target column.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Dictionary containing the train\n",
    "            and test dataframes.\n",
    "    \"\"\"\n",
    "    train_df, test_df = train_test_split(\n",
    "        raw_df.drop(columns=['duration']),\n",
    "        test_size=0.2,\n",
    "        stratify=raw_df[target_col],\n",
    "        random_state=42\n",
    "    )\n",
    "    return {'train': train_df, 'test': test_df}\n",
    "\n",
    "\n",
    "def create_inputs_and_targets(\n",
    "        df_dict: Dict[str, pd.DataFrame],\n",
    "        input_cols: list[str],\n",
    "        target_col: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create inputs and targets for training and test sets.\n",
    "\n",
    "    Args:\n",
    "        df_dict (Dict[str, pd.DataFrame]): Dictionary containing the train\n",
    "            and test dataframes.\n",
    "        input_cols (list): List of input columns.\n",
    "        target_col (str): Target column.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing inputs and targets\n",
    "            for train and test sets.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for split in df_dict:\n",
    "        data[f'{split}_inputs'] = df_dict[split][input_cols].copy()\n",
    "        data[f'{split}_targets'] = (\n",
    "            df_dict[split][target_col].map({'yes': 1, 'no': 0}).copy()\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_feature_types(df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Identify numerical and categorical columns in the dataframe.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include='number').columns.to_list()\n",
    "    categorical_cols = df.select_dtypes(include='object').columns.to_list()\n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "\n",
    "def feature_engineering(inputs: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply feature engineering transformations.\n",
    "    \"\"\"\n",
    "    inputs = inputs.copy()\n",
    "    inputs['age_cat'] = inputs['age'] // 10\n",
    "    inputs['is_only_one_contact'] = (inputs['campaign'] == 1).astype(int)\n",
    "    inputs['more_than_six_contacts'] = (inputs['campaign'] > 6).astype(int)\n",
    "    inputs['previous_contact'] = (inputs['previous'] > 0).astype(int)\n",
    "    inputs['recent_contact'] = (inputs['pdays'] < 7).astype(int)\n",
    "    inputs['pdays_3'] = (inputs['pdays'] == 3).astype(int)\n",
    "    inputs['pdays_6'] = (inputs['pdays'] == 6).astype(int)\n",
    "    inputs['is_hight_education'] = (inputs['education']\n",
    "                                    .isin(['university.degree',\n",
    "                                           'professional.course'])\n",
    "                                    .astype(int))\n",
    "    inputs['is_basic_education'] = (inputs['education']\n",
    "                                    .isin(['basic.4y', 'basic.9y',\n",
    "                                           'basic.6y'])\n",
    "                                    .astype(int))\n",
    "    inputs['nr.employed_to_emp.var.rate'] = (inputs['nr.employed']\n",
    "                                             / inputs['emp.var.rate'])\n",
    "    inputs['cons.price.idx_to_cons.conf.idx'] = (inputs['cons.price.idx']\n",
    "                                                 / inputs['cons.conf.idx'])\n",
    "    inputs['euribor3m_to_emp.var.rate'] = (inputs['euribor3m']\n",
    "                                           / inputs['emp.var.rate'])\n",
    "    inputs['cons.price.idx_to_emp.var.rate'] = (inputs['cons.price.idx']\n",
    "                                                / inputs['emp.var.rate'])\n",
    "    inputs['cons.price.idx_to_euribor3m'] = (inputs['cons.price.idx']\n",
    "                                             / inputs['euribor3m'])\n",
    "    inputs['cons.conf.idx_to_euribor3m'] = (inputs['cons.conf.idx']\n",
    "                                            / inputs['euribor3m'])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def get_train_test_data(\n",
    "        raw_df: pd.DataFrame,\n",
    "        target_col: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create inputs and targets for training and test sets.\n",
    "\n",
    "    Args:\n",
    "        df_dict (Dict[str, pd.DataFrame]): Dictionary containing the train\n",
    "            and test dataframes.\n",
    "        input_cols (list): List of input columns.\n",
    "        target_col (str): Target column.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing inputs and targets\n",
    "            for train and test sets.\n",
    "    \"\"\"\n",
    "    split_data_dict = split_data(raw_df, target_col)\n",
    "\n",
    "    input_cols = list(raw_df.drop(columns=['duration', 'y']).columns)\n",
    "    for split in split_data_dict:\n",
    "        split_data_dict[split] = feature_engineering(split_data_dict[split])\n",
    "    data_dict = create_inputs_and_targets(\n",
    "        split_data_dict, input_cols, target_col\n",
    "    )\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def get_preprocessor(inputs: pd.DataFrame,\n",
    "                     tree_base_model: bool = True) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline for numerical and categorical features.\n",
    "    \"\"\"\n",
    "    numeric_cols, categorical_cols = get_feature_types(inputs)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(\n",
    "        sparse_output=False, handle_unknown='ignore'\n",
    "    )\n",
    "\n",
    "    ordinal_mapping_education = [[\n",
    "        'unknown', 'illiterate', 'basic.4y', 'basic.6y',\n",
    "        'basic.9y', 'high.school', 'professional.course',\n",
    "        'university.degree']]\n",
    "\n",
    "    ordinal_enc = OrdinalEncoder(categories=ordinal_mapping_education)\n",
    "\n",
    "    if tree_base_model:\n",
    "        one_hot_enc_cols = [\n",
    "            'job', 'marital', 'default', 'housing', 'loan',\n",
    "            'contact', 'month', 'day_of_week', 'poutcome', 'previous']\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('onehot_enc', one_hot_encoder, one_hot_enc_cols),\n",
    "                ('ord_enc', ordinal_enc, ['education'])\n",
    "            ],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "        ).set_output(transform='pandas')\n",
    "    else:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', scaler, numeric_cols),\n",
    "                ('onehot_enc', one_hot_encoder, categorical_cols)\n",
    "            ],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "        ).set_output(transform='pandas')\n",
    "    preprocessor.fit(inputs)\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def preprocess_data(\n",
    "        raw_df: pd.DataFrame,\n",
    "        target_col: str,\n",
    "        tree_base_model: bool = True\n",
    "        ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Preprocess the raw dataframe.\n",
    "\n",
    "    Args:\n",
    "        raw_df (pd.DataFrame): The raw dataframe.\n",
    "        target_col (str): Target column.\n",
    "        scaler_numeric (bool): Whether to scale numeric features.\n",
    "            Default is True.\n",
    "        impute_strategy (str): Strategy for imputing missing values.\n",
    "            Default is 'median'.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing processed inputs and targets\n",
    "            for train and validation sets.\n",
    "    \"\"\"\n",
    "    split_data_dict = split_data(raw_df, target_col)\n",
    "\n",
    "    input_cols = list(raw_df.drop(columns=['duration', 'y']).columns)\n",
    "    data_dict = create_inputs_and_targets(\n",
    "        split_data_dict, input_cols, target_col\n",
    "    )\n",
    "    train_inputs = data_dict['train_inputs']\n",
    "    train_targets = data_dict['train_targets']\n",
    "\n",
    "    train_inputs = feature_engineering(train_inputs)\n",
    "    preprocessor = get_preprocessor(train_inputs, tree_base_model)\n",
    "    train_inputs_transform = preprocessor.transform(train_inputs)\n",
    "    return train_inputs_transform, train_targets\n",
    "\n",
    "\n",
    "def preprocess_new_data(\n",
    "        new_data: pd.DataFrame,\n",
    "        preprocessor: ColumnTransformer) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess new incoming data using the trained preprocessor.\n",
    "    \"\"\"\n",
    "    return preprocessor.transform(new_data)\n",
    "\n",
    "\n",
    "def get_auc(model, inputs: pd.DataFrame, targets):\n",
    "    \"\"\"\n",
    "    Compute AUROC score for rhe given model\n",
    "    \"\"\"\n",
    "    auc = cross_validate(model,\n",
    "                         X=inputs,\n",
    "                         y=targets,\n",
    "                         scoring='roc_auc',\n",
    "                         cv=3,\n",
    "                         return_train_score=True)\n",
    "    train_score = np.mean(auc['train_score'])\n",
    "    val_score = np.mean(auc['test_score'])\n",
    "    print(f\"AUROC score on train set:: {train_score:.3f}\")\n",
    "    print(f\"AUROC score on validation set:: {val_score:.3f}\")\n",
    "    return train_score, val_score\n",
    "\n",
    "\n",
    "def get_eval_results(preprocessor, clf, inputs, targets):\n",
    "    \"\"\"\n",
    "    Evaluate the model using AUROC\n",
    "    \"\"\"\n",
    "    # dictionary for saving results\n",
    "    results_dict = {'model_name': str(clf).split('(')[0],\n",
    "                    'params': str(clf).split('(')[1].rsplit(')')[0]}\n",
    "\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    train_score, val_score = get_auc(model_pipeline, inputs, targets)\n",
    "    results_dict['AUROC on train'] = train_score\n",
    "    results_dict['AUROC on validation'] = val_score\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "def get_confusion_matrix(model, inputs, targets):\n",
    "    \"\"\"\n",
    "    Generate and plot the Confusion matrix.\n",
    "    \"\"\"\n",
    "    preds = cross_val_predict(model, inputs, targets, cv=3)\n",
    "    cm = confusion_matrix(targets, preds, normalize='true')\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, cmap='YlOrBr',\n",
    "                xticklabels=['no', 'yes'], yticklabels=['no', 'yes'])\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
